{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DOF_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EbSxQkmkEkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9njCDJ0fEmA",
        "colab_type": "code",
        "outputId": "35f0a56a-8429-4282-cc0d-6fac4a8c4167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 863
        }
      },
      "source": [
        "with torch.autograd.detect_anomaly():\n",
        "\n",
        "  import numpy as np\n",
        "  import torch\n",
        "  import torch.nn as nn\n",
        "  import torch.optim as optim\n",
        "  from torch.autograd import Variable\n",
        "  import torch.utils as utils\n",
        "  import torchvision.datasets as dset\n",
        "  import torchvision.transforms as transforms\n",
        "  import matplotlib.pyplot as plt\n",
        "  from torchsummary import summary\n",
        "\n",
        "  device = torch.device('cuda')\n",
        "\n",
        "  def pose_loss(output_1,output_2, target_1, target_2):\n",
        "      with torch.no_grad():\n",
        "        P = torch.dot(output_1, output_2)\n",
        "        P_truth = torch.dot(target_1, target_2)\n",
        "        loss = (P - P_truth)**2\n",
        "        return loss \n",
        "\n",
        "  def now_loss(output_1,target_1):\n",
        "      return torch.mean((output_1 - target_1) ** 2)\n",
        "\n",
        "  def my_loss(out, tar):\n",
        "      loss = 0\n",
        "      loss = loss + pose_loss(out[0],out[1],tar[0],tar[1])\n",
        "      loss = loss + pose_loss(out[1],out[2],tar[1],tar[2])\n",
        "      loss = loss + pose_loss(out[2],out[3],tar[2],tar[3])\n",
        "      loss = loss + pose_loss(out[3],out[4],tar[3],tar[4])\n",
        "      loss = loss + pose_loss(out[0],out[2],tar[0],tar[2])\n",
        "      loss = loss + pose_loss(out[2],out[4],tar[2],tar[4])\n",
        "      loss = loss + pose_loss(out[0],out[4],tar[0],tar[4]) \n",
        "      loss = loss + now_loss(out[0],tar[0])\n",
        "      loss = loss + now_loss(out[1],tar[1])\n",
        "      loss = loss + now_loss(out[2],tar[2])\n",
        "      loss = loss + now_loss(out[3],tar[3])\n",
        "      loss = loss + now_loss(out[4],tar[4])\n",
        "      return loss\n",
        "\n",
        "  class Tobi_model(nn.Module):\n",
        "      def __init__(self):\n",
        "          super(Tobi_model, self).__init__()\n",
        "          self.Res = ResNet50()\n",
        "          self.Res_5 = res_5()\n",
        "          self.Res_5_2 = res_5_2()\n",
        "          self.ELU = nn.ELU()\n",
        "          self.rnn = nn.LSTM(input_size=int(98304/24), hidden_size=1000, num_layers=2, batch_first=True)\n",
        "          self.rnn_2 = nn.LSTM(input_size=int(98304/24), hidden_size=1000, num_layers=1, batch_first=True)\n",
        "          self.fc1 = nn.Linear(1000, 1024)\n",
        "          self.fc2 = nn.Linear(2048, 1024)\n",
        "          self.fc3 = nn.Linear(1024, 1024)\n",
        "          self.final_1 = nn.Linear(1024, 3)\n",
        "          self.final_2 = nn.Linear(1024, 4)\n",
        "\n",
        "      def forward(self, x):\n",
        "          x_1 = x[0].unsqueeze(0).clone()  # t-1  step\n",
        "          x_2 = x[1].unsqueeze(0).clone() # t step\n",
        "          \n",
        "          #Residual block pass \n",
        "          x_1 = self.Res(x_1) \n",
        "          x_2 = self.Res(x_2)\n",
        "\n",
        "          #RCNN1\n",
        "          x_3 = torch.cat([x_1,x_2],dim = 1)\n",
        "          x_3 = self.Res_5_2(x_3)\n",
        "          # x_3 = nn.functional.avg_pool2d(x_3, 4)\n",
        "          x_3, h_c = self.rnn(x_3)\n",
        "          x_3 = self.fc1(x_3)\n",
        "          #RCNN2\n",
        "          x_2 = self.Res_5(x_2)\n",
        "          # x_2 = nn.functional.avg_pool2d(x_2, 4)\n",
        "\n",
        "          x_2, h_c_2 = self.rnn_2(x_2)\n",
        "          x_2 = self.fc1(x_2)\n",
        "          x_2 = self.ELU(x_2)\n",
        "          #FC layer\n",
        "          x_2 = x_2.view(1,1024)\n",
        "          x_3 = x_3.view(1,1024)\n",
        "          x_3 = torch.cat([x_2,x_3],dim = 1)\n",
        "          # print(x_2.size())\n",
        "          # print(x_3.size())\n",
        "          x_3 = self.fc2(x_3)\n",
        "\n",
        "          #Translation\n",
        "          x_4 = self.fc3(x_3)\n",
        "          x_4 = self.final_1(x_4)\n",
        "\n",
        "          #Quanternion\n",
        "          x_5 = self.fc3(x_3)\n",
        "          x_5 = self.final_2(x_3)\n",
        "          out = torch.cat([x_4,x_5], dim = 1)\n",
        "          return out\n",
        "\n",
        "      # def forward(self, x)\n",
        "\n",
        "\n",
        "  ## ResNet Block\n",
        "  class ResNet(nn.Module):\n",
        "\n",
        "      def __init__(self, block, num_blocks):\n",
        "          super(ResNet, self).__init__()\n",
        "          self.in_planes = 64 \n",
        "          self.conv1 = nn.Conv2d(3, 64, kernel_size=3,stride=1, padding=1, bias=False)\n",
        "          self.bn1 = nn.BatchNorm2d(64)\n",
        "          self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "          self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "          self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "          self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "          self.linear = nn.Linear(2048, 1024)\n",
        "          self.ELU = nn.ELU(inplace=False)\n",
        "\n",
        "\n",
        "      def _make_layer(self, block, planes, num_blocks, stride):\n",
        "          strides = [stride] + [1]*(num_blocks-1)\n",
        "          layers = []\n",
        "          for stride in strides:\n",
        "              layers.append(block(self.in_planes, planes, stride))\n",
        "              self.in_planes = planes * block.expansion\n",
        "          return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "      def forward(self, x): # 4번째 layer 및 ave_pool은 하지 않음\n",
        "          out = self.conv1(x) \n",
        "          out = self.bn1(out)\n",
        "          out = self.ELU(out)\n",
        "          out = self.layer1(out)\n",
        "          out = self.layer2(out)\n",
        "          out = self.layer3(out)\n",
        "  #         out = self.layer4(out)  \n",
        "  #         out = nn.functional.avg_pool2d(out, 4)\n",
        "          return out\n",
        "\n",
        "\n",
        "  #Bottleneck 구조\n",
        "  class Bottleneck(nn.Module):\n",
        "      expansion = 4\n",
        "      def __init__(self, in_planes, planes, stride=1):\n",
        "          super(Bottleneck, self).__init__()\n",
        "          self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "          self.bn1 = nn.BatchNorm2d(planes)\n",
        "          self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                                stride=stride, padding=1, bias=False)\n",
        "          self.bn2 = nn.BatchNorm2d(planes)\n",
        "          self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                                planes, kernel_size=1, bias=False)\n",
        "          self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "          self.shortcut = nn.Sequential()\n",
        "          if stride != 1 or in_planes != self.expansion*planes:\n",
        "              self.shortcut = nn.Sequential(\n",
        "                  nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                            kernel_size=1, stride=stride, bias=False),\n",
        "                  nn.BatchNorm2d(self.expansion*planes)\n",
        "              )\n",
        "          self.elu = nn.ELU(inplace = True) # ELU 추가됨\n",
        "      #Bottle Neck 한 덩어리 이게 3,4,6,3번 반복되면 Resnet 50!\n",
        "\n",
        "      def forward(self, x):\n",
        "          out = self.elu(self.bn1(self.conv1(x)))\n",
        "          out = self.elu(self.bn2(self.conv2(out)))\n",
        "          out = self.bn3(self.conv3(out))\n",
        "          out = out + self.shortcut(x)\n",
        "          out = self.elu(out)\n",
        "          return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # 5번째 ResNet 블럭\n",
        "  class ResNet_5(nn.Module):\n",
        "      def __init__(self, block):\n",
        "          super(ResNet_5, self).__init__()\n",
        "          self.in_planes = 1024\n",
        "          self.conv1 = nn.Conv2d(512, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "          self.bn1 = nn.BatchNorm2d(64)        \n",
        "          self.layer1_ = self._make_layer(block, 512, 3, stride=2)\n",
        "          self.ELU = nn.ELU()\n",
        "          self.linear = nn.Linear(2048, 1024)\n",
        "          \n",
        "      def _make_layer(self, block, planes, num_blocks, stride):\n",
        "          strides = [stride] + [1]*(num_blocks-1)\n",
        "          layers = []\n",
        "          for stride in strides:\n",
        "              layers.append(block(self.in_planes, planes, stride))\n",
        "              self.in_planes = planes * block.expansion\n",
        "          return nn.Sequential(*layers)\n",
        "\n",
        "      # 5번쨰 Resdual BLock후, 펴주자\n",
        "      def forward(self, x):\n",
        "          out = self.layer1_(x)\n",
        "          out = self.ELU(out)\n",
        "          out = nn.functional.avg_pool2d(out, 16)\n",
        "          out = out.view(1,out.size(0), -1)\n",
        "          # out = self.linear(out)\n",
        "          return out\n",
        "\n",
        "  class ResNet_5_2(nn.Module):\n",
        "      def __init__(self, block):\n",
        "          super(ResNet_5_2, self).__init__()\n",
        "          self.in_planes = 2048\n",
        "          self.conv1 = nn.Conv2d(512, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "          self.bn1 = nn.BatchNorm2d(64)        \n",
        "          self.layer1_ = self._make_layer(block, 512, 3, stride=2)\n",
        "          self.ELU = nn.ELU()\n",
        "          self.linear = nn.Linear(2048, 1024)\n",
        "          \n",
        "      def _make_layer(self, block, planes, num_blocks, stride):\n",
        "          strides = [stride] + [1]*(num_blocks-1)\n",
        "          layers = []\n",
        "          for stride in strides:\n",
        "              layers.append(block(self.in_planes, planes, stride))\n",
        "              self.in_planes = planes * block.expansion\n",
        "          return nn.Sequential(*layers)\n",
        "\n",
        "      # 5번쨰 Resdual BLock후, 펴주자\n",
        "      def forward(self, x):\n",
        "          out = self.layer1_(x)\n",
        "          out = self.ELU(out)\n",
        "          out = nn.functional.avg_pool2d(out, 16)\n",
        "          out = out.view(1,out.size(0), -1)\n",
        "          # out = self.linear(out)\n",
        "          return out\n",
        "\n",
        "  def res_5():\n",
        "      return  ResNet_5(Bottleneck)\n",
        "\n",
        "  def res_5_2():\n",
        "      return  ResNet_5_2(Bottleneck)\n",
        "\n",
        "  def ResNet50():\n",
        "      return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "  def test():\n",
        "      net = ResNet18()\n",
        "      y = net(torch.randn(1, 3, 32, 32))\n",
        "      # print(y.size())\n",
        "\n",
        "\n",
        "  class loss_cal(nn.Module):\n",
        "      def __init__(self,label):\n",
        "          super(loss_cal, self).__init__()\n",
        "          self.out_tensor=torch.ones(5,7)\n",
        "          self.label = label\n",
        "          self.num = 0\n",
        "\n",
        "      def add_loss(self, out_):\n",
        "          for i in range(5):\n",
        "              if i!=4:\n",
        "                  self.out_tensor[i] = self.out_tensor[i+1].clone()\n",
        "              else:\n",
        "                  # print(out_.size())\n",
        "                  self.out_tensor[i] = out_\n",
        "          self.num = self.num + 1\n",
        "      \n",
        "      def calculate_loss(self, out__):\n",
        "          if self.num>=5:\n",
        "              self.add_loss(out__)\n",
        "              loss = my_loss(self.out_tensor,self.label[self.num:self.num+5,:].clone())\n",
        "              return loss\n",
        "          else:\n",
        "              self.add_loss(out__)\n",
        "              pass\n",
        "\n",
        "\n",
        "  print('before concat sample')\n",
        "  ###############################\n",
        "  ## test tensor concat sample ##\n",
        "  ###############################\n",
        "  data_size = 64\n",
        "  batch_size = 1\n",
        "  input = torch.rand(data_size,1,3,256,192,requires_grad=True).to(device)\n",
        "  # input = input.int()\n",
        "\n",
        "  resnet1_4 = ResNet50()\n",
        "  resnet5 = res_5()\n",
        "\n",
        "  pre_out_ = 0\n",
        "  out_ = 0\n",
        "  fin_out_ = 0\n",
        "  im_data_stack = []\n",
        "  im_data_stack_2 = []\n",
        "\n",
        "  label = torch.ones(data_size,7)\n",
        "\n",
        "  loss_ = loss_cal(label)\n",
        "\n",
        "  # for i in range(int(data_size/batch_size)):\n",
        "  #     if i==0:\n",
        "  #         im_data_stack.append(input[i])\n",
        "  #         pre_out_ = resnet1_4.forward(im_data_stack[0])\n",
        "  #         pre_out_ = resnet5.forward(pre_out_)\n",
        "  #     elif i==1:\n",
        "  #         im_data_stack.append(input[i])\n",
        "  #         out_ = resnet1_4.forward(im_data_stack[1])\n",
        "  #         out_ = resnet5.forward(out_)\n",
        "  #         fin_out_ = torch.cat([pre_out_,out_],dim=0)\n",
        "  #     else:\n",
        "  #         pre_out_ = out_\n",
        "  #         del im_data_stack[0]\n",
        "  #         im_data_stack.append(input[i])\n",
        "  #         out_ = resnet1_4.forward(im_data_stack[1])\n",
        "  #         out_ = resnet5.forward(out_)\n",
        "  #         fin_out_ = torch.cat([pre_out_,out_],dim=0)\n",
        "  #         print(fin_out_.size()) \n",
        "\n",
        "  print('before Tobi model')\n",
        "  # tobiVO = Tobi_model().to('cpu')\n",
        "  tobiVO = Tobi_model().to(device)\n",
        "  cal_loss = 0\n",
        "  torch.save(tobiVO,'./modelsize')\n",
        "  # print(tobiVO.eval())\n",
        "  # summary(tobiVO,input_size=(3,256,192),device='cpu')\n",
        "\n",
        "  optimizer = optim.Adam(tobiVO.parameters(), lr=0.001)\n",
        "  print('before Tobi model')\n",
        "  # with torch.no_grad():\n",
        "  for i in range(input.size()[0]):\n",
        "      temp_loss = 0\n",
        "      if i>=1:\n",
        "          optimizer.zero_grad()\n",
        "          input_ = torch.cat([input[i-1],input[i]],dim=0)\n",
        "          finout = tobiVO.forward(input_)\n",
        "          print(loss_.num)\n",
        "\n",
        "          if(loss_.num>=5):\n",
        "              # print('hi')\n",
        "              temp_loss = loss_.calculate_loss(finout)\n",
        "              cal_loss = cal_loss + temp_loss\n",
        "              cal_loss.backward(retain_graph = True)\n",
        "              optimizer.step()\n",
        "              print(cal_loss)\n",
        "          else:\n",
        "              with torch.no_grad():\n",
        "                loss_.calculate_loss(finout)\n",
        "          \n",
        "  print('all ran')    \n",
        "  print(type(cal_loss))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/autograd/anomaly_mode.py:70: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "  warnings.warn('Anomaly Detection has been enabled. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "before concat sample\n",
            "before Tobi model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Tobi_model. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Bottleneck. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type ResNet_5. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type ResNet_5_2. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "before Tobi model\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "tensor(347.8582, grad_fn=<AddBackward0>)\n",
            "6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7a5f0677db97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    329\u001b[0m               \u001b[0mtemp_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m               \u001b[0mcal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcal_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtemp_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m               \u001b[0mcal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [1024, 4]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
          ]
        }
      ]
    }
  ]
}